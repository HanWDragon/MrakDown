- Scaling Database & Consistent Hashing

# Interviewer：How to scale?

- 当访问量越来越大以后，如何让你的系统 Scale?
- How to scale system ≈ How to scale database

## 除了QPS，还有什么需要考虑的?

- 假如用户的 QPS 有 1k
- Database 和 Web Server 假设也均能够承受 1k 的 QPS
- 还有什么情况需要考虑？
	- 假如你是公司的顶梁柱，你一天能工作 16 小时，但是你不会生病吗？不会有其他的事情？

## 单点失效 Single Point Failure

- 万一这一台数据库挂了 
- 短暂的挂
	- 网站就不可用了
- 彻底的挂
	- 数据就全丢了
- 通常任何类型的系统，机器都不会只有一台
- 但是 Web Server 增加机器其实没有太大的难度，
	- 因为 Web Server 是一种 StateLess 的服务
	- 只要网关设置好，流量分配均衡就行了
	- 通常 Web Server 又不会储存你的信息，真正储存你信息都是后台的数据库和缓存
- 所以我们增加数据库要考虑几个事情

## 为了增加数据库你需要做两件事情

1. 数据拆分 Sharding(又名 Partition)
	- 拆分之后要是数据库挂了，也会导致用户信息丢失
2. 数据复制 Replica
	- 在不同的机器上多存几份

- 其实最难的就是数据拆分

### 数据拆分 Sharding

- 又名 Partition
- 方法
	- 按照一定的规则，将数据拆分开存储在不同的实体机器上
- 意义
	1. 挂了一台不会全挂
	2. 分摊读写流量

- 这个可不能随便放的，假如用户要查询信息，我要是不知道放在哪里，我去不是要一台一台的找，那干嘛数据拆分。
- 我数据拆分是要将流量也拆分掉
- 所以需要拆分，即使挂了一个节点，也能保证一定的可用性

### 数据复制 Replica

- 方法
	- 一式三份(重要的事情写三遍)
- 意义
	1. 一台机器挂了可以用其他两台的数据恢复
	2. 分摊读请求

- 统计数据上看，两台机器挂掉的概率还是很大
- 当三台挂掉的概率就比较低了，而且为了预防地质灾害，通常服务器也不是放在一起的，即使你到了别的地方，你的数据访问依然很快，你通常访问的都是离你最近的一台机器

## Sharding in SQL vs NoSQL

- 数据拆分与数据库的类型无关  
- 无论是 SQL 还是 NoSQL 都可以进行数据拆分
- 大部分的 NoSQL 已经帮你写好了拆分算法

## 数据拆分 Sharding

### 纵向拆分 Vertical Sharding

- User Table 放一台数据库
- Friendship Table 放一台数据库
- Message Table 放一台数据库
- ...

#### 稍微复杂一点的 Vertical Sharding

- 比如你的 User Table 里有如下信息
	- email
	- username  
	- password  
	- nickname // 昵称
	- avatar // 头像
- 我们知道 email / username / password 不会经常变动
- 而 nickname, avatar 相对来说变动频率更高
- 可以把他们拆分为两个表 User Table 和 UserProfile Table
	- UserProfile 中用一个 user_id 的 foreign key 指向 User  
	- 然后再分别放在两台机器上
	- 这样如果 UserProfile Table 挂了，就不影响 User 正常的登陆

#### 提问

- Vertial Sharding 的缺点是什么? 
	- 表太大了，访问量也大
	- 这个表就两行

### 横向拆分 Horizontal Sharding

- 核心部分!
- Scale 的核心考点!

#### 方案1：新数据放新机器，旧数据放旧机器

- 比如一台数据库如果能放下 1T 的数据
- 那么超过1T之后就放在第二个数据库里
- 以此类推 
- 这种方法的问题是什么?
	1. 时间局部性，系统中越新的数据访问的越多，但是这个假设认为所有的数据访问概率是一样的
	2. 导致新机器访问量过大，很多人新发的数据，都会有很多人来查看的，你一年之前写的一些文章，一个月之内都没人看

#### 猜想2：对机器数目取模

- 假如我们来拆分 Friendship Table
- 我们有 3 台数据库的机器
- 于是想到按照 from_user_id % 3 进行拆分
- 这样做的问题是啥?
	- 当数据量进一步提升，要加新机器了，数据迁移咋办？

##### 假如 3 台机器不够用了

- 我现在新买了 1 台机器
- 原来的%3，就变成了%4
- 几乎所有的数据都要进行位置大迁移

##### 过多的数据迁移会造成的问题

1. 慢，容易造成数据的不一致性  
2. 迁移期间，服务器压力增大，容易挂

##### 直接 %n (机器总数)的方法

- % n 的方法是一种最简单的 Hash 算法  
	- 但是这种方法在 n 变成 n+1 的时候，每个 key % n和 % (n+1) 结果基本上都不一样
	- 所以这个 Hash 算法可以称之为：不一致 hash

![](image/Pasted%20image%2020221012215751.png)

#### 如何解决

- 一致性 Hash 算法
- Consistent Hashing

#### 一种简单的一致性哈希算法

- Consistent Hashing

- Horizontal Sharding 的秘密武器
- 无论是 SQL 还是 NoSQL 都可以用这个方法进行 Sharding
- 注：大部分 NoSQL 都帮你实现好了这个算法，帮你自动 Sharding
- 很多 SQL 数据库也逐渐加入 Auto-scaling 的机制了，也开始帮你做 自动的 Sharding

- 我们先来说说为什么要做 “一致性”Hash
	- % n 的方法是一种最简单的 Hash 算法  
	- 但是这种方法在 n 变成 n+1 的时候，每个 key % n和 % (n+1) 结果基本上都不一样
	- 所以这个 Hash 算法可以称之为：不一致 hash

![](image/Pasted%20image%2020221016135112.png)

- 一个简单的一致性Hash算法
	- 将 key 模一个很大的数，比如 360
	- 将 360 分配给 n 台机器，每个机器负责一段区间
	- 区间分配信息记录为一张表存在 Web Server 上
	- 新加一台机器的时候，在表中选择一个位置插入，匀走相邻两台机器的一部分数据
	- 在圆环上找两个相邻区间，这两个相邻区间的总和是最大的，然后将这个区域三等分，这样就完成了添加区间
- 比如n从2变化到3，只有1/3的数据移动

![](image/Pasted%20image%2020221016135217.png)

##### 3台机器变4台机器的例子

![](image/Pasted%20image%2020221016135826.png)

##### 缺陷1 数据分布不均匀

- 因为算法是“将数据最多的相邻两台机器均匀分为三台”
- 比如，3台机器变4台机器时，无法做到4台机器均匀分布
- 假如每台机器都忙，你加入了一台机器用来分担压力，但是这个机器只去分担最忙的两个机器，这个肯定是不符合预期的

##### 缺陷2 迁移压力大

- 新机器的数据只从两台老机器上获取
- 导致这两台老机器负载过大
- 我们想要的是每个机器都分担一些迁移的压力

#### 哈希函数 Hash Function

- 将任意类型的 key 转换为一个 0~size-1 的整数
- 在 Consistent Hashing 中，一般取 size = 2^64
- 很多哈希算法可以保证不同的 key 算得相同的 hash 值的概率
- 等于宇宙爆炸的概率

#### 更完美的的一致性哈希算法

- 将取模的底数从 360 拓展到 $2^{64}$
- 将 0~2^64-1 看做一个很大的圆环(Ring)
- 将数据和机器都通过 hash function 换算到环上的一个点
	- 数据取 key / row_key 作为 hash key  
	- 机器取MAC地址，或者机器固定名字如 database01，或者固定的 IP 地址  
	- 每个数据放在哪台机器上，取决于在 Consistent Hash Ring 上顺时针碰到的 下一个机器节点
- 但是这个算法还是没有解决均匀性的问题，因为通过 Hash Function 算出来的结果，只会是一个很乱的结果，所谓的均匀也只是在大数据量情况下，一种统计事实
- 所以我们就引入了虚拟节点

![](image/Pasted%20image%2020221017154348.png)

##### 虚拟节点 Virtual Node

- 引入分身的概念(Virtual nodes)
- 一个实体机器(Real node) 对应若干虚拟机器(Virtual Node)
- 通常是 1000 个
- 这样就让之前的一台机器只占据一个区间，变成了很多区间，这样就保证了均匀性
- 分身的KEY可以用**实体机器的KEY+后缀**的形式
	- 如 database01-0001
	- 好处是直接按格式去掉后缀就可以得到实体机器
- 用一个数据结构存储这些 virtual nodes
	- 支持快速的查询比某个数大的最小数
		- 即顺时针碰到的下一个 virtual nodes
	- 还要满足频繁的增删查改的需求
	- 时间复杂度还要尽可能低
	- 哪种数据结构可以支持?
		- 红黑树

![](image/Pasted%20image%2020221017155011.png)

##### 新增一台机器

- 创建对应的 1000 个分身 db99-000 ~ 999
- 加入到 virtuals nodes 集合中
- 问：该从哪些机器迁移哪些数据到新机器上?
	- 一个机器出发能顺时针碰到新机器的那段区间迁移数据

# Replica 数据备份

- Backup 和 Replica 有什么区别

## Backup vs Replica

### Backup

- 一般是周期性的，比如每天晚上进行一次备份  
- 当数据丢失的时候，通常只能恢复到之前的某个时间点 
- Backup 不用作在线的数据服务，不分摊读
  
### Replica    

- 是实时的， 在数据写入的时候，就会以复制品的形式存为多份  
- 当数据丢失的时候，可以马上通过其他的复制品恢复 
- Replica 用作在线的数据服务，分摊读

思考:既然 Replica 更牛，那么还需要 Backup么?
- 机器总会出问题的，要防着

## MySQL Replica

- 以MySQL为代表SQL型数据库，通常“自带” Master Slave 的 Replica 方法
- Master 负责写，Slave 负责读
- Slave 从 Master 中同步数据

### Master - Slave

- 原理
	- Write Ahead Log
	- SQL 数据库的任何操作，都会以 Log 的形式做一份记录 • 比如数据A在B时刻从C改到了D
	- Slave 被激活后，告诉master我在了
	- Master每次有任何操作就通知 slave 来读log
	- 因此Slave上的数据是有“延迟”的
- Master 挂了怎么办?  
	- 将一台 Slave 升级 (promote) 为 Master，接受读+写
	- 可能会造成一定程度的数据丢失和不一致

![](image/Pasted%20image%2020221017164133.png)



## NoSQL Replica

- 以 Cassandra 为代表的 NoSQL 数据库
- 通常将数据“顺时针”存储在 Consistent hashing 环上的三个 virtual nodes 中

## SQL vs NoSQL in Replica

- SQL  
	- “自带” 的 Replica 方式是 Master Slave  
	- “手动” 的 Replica 方式也可以在 Consistent Hashing 环上顺时针存三份
- NoSQL  
	- “自带” 的 Replica 方式就是 Consistent Hashing 环上顺时针存三份
	- “手动” 的 Replica 方式 就不需要手动了，NoSQL就是在 Sharding 和 Replica 上帮你偷懒用的!

# 实战1 User Table 如何 Sharding

- 如果我们在 SQL 数据库中存储 User Table
- 那么按照什么做 Sharding ?

## 怎么取数据就怎么拆数据

- How to shard data based on how to query data
- 绝大多数请求
	- `select * from user_table where user_id=xxx`
	- 那就是使用 user_id 拆分
- 如果我需要按照 username 找用户怎么办
	- 我们使用多个表单就行，使用一个新的表单，存储的是 username to user_id，多查询一遍不就好了
- 但是插入数据，就遇到了问题

## User Table Sharding 的几个问题

- User Table Sharding 之后，多台数据库无法维护一个全局的**自增ID**怎么办? 
	- 手动创建一个 UUID 来作为用户的 user_id
- 创建用户时还没有用户的 user_id，如何知道该去哪个数据库创建呢?  
    - Web Server 负责创建用户的 user_id，如用 UUID 来作为 user_id  
    - 创建之后根据 consistent_hash(user_id) 的结果来获得所在的实体数据库信息   
- 更进一步的问题:如果 User Table 没有 sharding 之前已经采用了自增ID该怎么办?
    - UUID 通常是一个字符串，自增 id 是一个整数，并不兼容 
    - 单独用一个 UserIdService 负责创建用户的 ID，每次有新用户需要创建时，问这个 Service 要一个新的ID
    - 这个 Service 是全局共享的，专门负责创建 UserId
    - 负责记录当前 UserId 的最大值是多少了，然后每次 +1 即可
    - 这个 Service 会负责加锁来保证数据操作的原子性(Atomic)
    - 因为创建用户不是一个很大的 QPS，因此这个做法问题不大

# 实战2 Friendship Table 如何 Sharding

- 双向好友关系是否还能只存储为一条数据?
	- 不行，如果分割了之后，一条数据不好找
	- 只能变成两条数据存储
- 单向好友关系按照什么 sharding?
	- 如何去取数据
	- 我们常见的查询就是我关注了谁，谁关注了我
	- 同样的道理，变成两条数据存储
	- 一条数据是关注信息，key 是 from_userid
	- 一条数据是被关注信息，key 就是 to_userid

 # 实战3 Session Table 如何 Sharding

- Session Table 主要包含 session_key(session token),user_id, expire_at 这几项
- 我们查询主要是通过 session_key，所以就使用 session_key

# 实战4 News Feed / Timeline 按照什么 Sharding

- News Feed = 新鲜事列表
- Timeline = 某人发的所有帖子

- 如何查询，就如何拆分
- own_id 新鲜事
- user_id Timeline

# 实战5 LintCode Submission 按照什么 Sharding

- Submission 包含了，谁 (user)什么时候(timestamp)提交了哪个题 (problem)得到了什么判定结果(status)

- 需求1 查询某个题的所有提交记录
	- `select * from submission_table where problem_id=1001;`
- 需求2 查询某个人的所有提交记录  
	- `select * from submission_table where user_id=101;`
- 单纯按照 user_id 或者 problem_id sharding 都无法同时满足两个查询需求。
- 解决办法(两个表单)
	- submission_table 按照 user_id sharding，因为按照 user_id 的查询操作相对频繁一些
	- 在 submission_table 之外再建立一张表单，记录某个题有哪些提交记录，表单包含 `<problem_id, user_id, submission_id, ...>` 等信息，以 problem_id 作为 sharding key`